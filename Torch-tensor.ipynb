{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量\n",
    "### 1.概念\n",
    "        (1)标量（0阶张量）: 只有大小，没有方向。\n",
    "        (2)向量（1阶张量）: \n",
    "                (2.1)笛卡尔坐标系的概念: 基向量（大小为1，方向为坐标轴方向）\n",
    "                (2.2)明确基向量表示向量的方法\n",
    "\n",
    "        m维张量可以看作m-1维张量在某个方向上的堆叠\n",
    "### 2.基本处理\n",
    "        (1)flatten:张量(c,w,h),start_dim = 0, end_dim = 1 ---> (cw,h)   (3,5,6,7) ---> (15,6,7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Pytorch中常见的张量生成方法如下，大部分操作与Numpy中的操作指令类似，如果构建时不声明张量的数据类型。一般会默认为float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3])\n",
      "tensor([[-2.2414e-36,  4.5846e-41, -9.0693e+11],\n",
      "        [ 3.0618e-41,  9.7643e+26,  4.5846e-41],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[0.9518, 0.7579, 0.4954],\n",
      "        [0.2569, 0.8091, 0.2045],\n",
      "        [0.7025, 0.2021, 0.4296]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#构造一个默认Float型的张量\n",
    "tensor_1 = torch.tensor((3,3))\n",
    "print(tensor_1)\n",
    "\n",
    "#构造一个无初始化定义的张量\n",
    "tensor_2 = torch.empty((3,3))\n",
    "print(tensor_2)\n",
    "\n",
    "#构造一个随机初始化的矩阵（值在0到1之间）\n",
    "tensor_3 = torch.rand((3,3))\n",
    "print(tensor_3)\n",
    "\n",
    "#构造一个0元素矩阵\n",
    "tensor_4 = torch.zeros((3,3))\n",
    "print(tensor_4)\n",
    "\n",
    "#构造一个元素全为1的矩阵\n",
    "tensor_5 = torch.ones((3,3))\n",
    "print(tensor_5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Pytorch中提供的与Numpy数据之间的相互转换，转换过程不影响我们的数据类型，数据类型必须保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9406, -1.1957,  1.0582],\n",
      "        [-1.1807,  0.0202, -0.0681],\n",
      "        [-0.1489, -1.3617,  0.0595]]) \n",
      " [[ 0.9405561  -1.1956948   1.0581717 ]\n",
      " [-1.1807301   0.02020014 -0.0681175 ]\n",
      " [-0.14894292 -1.3616508   0.05954116]]\n",
      "[[2 3]\n",
      " [4 5]\n",
      " [6 7]] \n",
      " tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#tensor转numpy\n",
    "t1 = torch.randn((3,3))\n",
    "#构建3X3元素的随机的tensor\n",
    "t1_numpy = t1.numpy()\n",
    "print(t1, \"\\n\", t1_numpy)\n",
    "#numpy转换为tensor\n",
    "#构建numpy格式的矩阵\n",
    "t1_numpy = np.array([[2,3],[4,5],[6,7]])\n",
    "t1_torch = torch.from_numpy(t1_numpy)\n",
    "print(t1_numpy, \"\\n\", t1_torch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Pytorch中的张量基本操作\n",
    "        形状查看及更改：在张量做加减乘除等运算时，需要保证张量的形状一致，往往需要对某些张量进行更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "t1 = torch.randn((3,3)) # 构建 3 × 3 元素随机的 tensor\n",
    "shape = t1.shape # 返回张量的维度 torchsize([3,3]), 也可以通过 tl size() 查看\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9577, -0.5508,  0.0897],\n",
      "        [-1.4344, -1.6909, -0.3070],\n",
      "        [ 0.0591, -1.0154, -1.0140]])\n",
      "tensor([[-0.9577, -0.5508,  0.0897, -1.4344, -1.6909, -0.3070,  0.0591, -1.0154,\n",
      "         -1.0140]])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "t1_reshape = t1.reshape(1,9)\n",
    "print(t1_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9577, -0.5508,  0.0897],\n",
      "        [-1.4344, -1.6909, -0.3070],\n",
      "        [ 0.0591, -1.0154, -1.0140]])\n",
      "tensor([[-0.9577, -0.5508,  0.0897, -1.4344, -1.6909, -0.3070,  0.0591, -1.0154,\n",
      "         -1.0140]])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "t1_resize = t1.resize(1,9) # 重构成 torch.size([1,9]),resize有两种使用方式，一种是没有返回值的，直接对原始的数据进行修改，还有一种用法是有返回值的，所以不会修改原有的数组值。,reshape不会修改原始数据的数据，而是返回一个新的数组\n",
    "print(t1_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "X_new:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.array([[1,2,3,4],\n",
    "              [5,6,7,8],\n",
    "              [9,10,11,12]])\n",
    " \n",
    "X_new=np.resize(X,(3,3)) # do not change the original X\n",
    "print(\"X:\\n\",X)  #original X\n",
    "print(\"X_new:\\n\",X_new) # new X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "X_2:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X=np.array([[1,2,3,4],\n",
    "              [5,6,7,8],\n",
    "              [9,10,11,12]])\n",
    "\n",
    "X_2=X.resize((3,3))  #change the original X ,and do not return a value\n",
    "print(\"X:\\n\",X)  # change the original X\n",
    "print(\"X_2:\\n\",X_2) # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9609, -0.4447,  0.7242],\n",
      "        [-0.3501, -1.1566, -0.7684],\n",
      "        [-0.9761, -0.7472,  0.1473]])\n",
      "tensor([-0.9609, -0.4447,  0.7242, -0.3501, -1.1566, -0.7684, -0.9761, -0.7472,\n",
      "         0.1473])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((3,3))\n",
    "print(t1)\n",
    "t1_view = t1.view(-1)# 拉伸成一维张量 torchsize([9J\n",
    "print(t1_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0801, -0.0322,  1.2713],\n",
      "        [ 0.0085, -0.7158,  0.3796],\n",
      "        [-0.7162,  0.1358,  0.1996]])\n",
      "tensor([[[-0.0801, -0.0322,  1.2713],\n",
      "         [ 0.0085, -0.7158,  0.3796],\n",
      "         [-0.7162,  0.1358,  0.1996]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((3,3))\n",
    "print(t1)\n",
    "t1_unsqueeze = torch.unsqueeze(t1,dim=0) # 增添维度 ， 结果为 torch.size([1,3,3])\n",
    "print(t1_unsqueeze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0801, -0.0322,  1.2713],\n",
      "        [ 0.0085, -0.7158,  0.3796],\n",
      "        [-0.7162,  0.1358,  0.1996]])\n"
     ]
    }
   ],
   "source": [
    "t1_squeeze = torch.squeeze(t1_unsqueeze,dim=0) # 删减维度结果为 torch.size([3])\n",
    "print(t1_squeeze)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Flatten:张量(c,w,h),start_dim = 0, end_dim = 1 ---> (cw,h)   (3,5,6,7) ---> (15,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7985, -1.3607, -0.0365],\n",
      "        [-0.7026, -1.5255,  0.8040],\n",
      "        [ 0.5479,  0.3249, -0.1690]])\n",
      "tensor([ 0.7985, -1.3607, -0.0365, -0.7026, -1.5255,  0.8040,  0.5479,  0.3249,\n",
      "        -0.1690])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((3,3))\n",
    "print(t1)\n",
    "t1_flatten = torch.flatten(t1) # 将两个维度间的元素进行拉伸 ， 结果为 torchsize([9])\n",
    "print(t1_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2457, -0.7202,  0.5922,  1.4378],\n",
      "         [ 0.4030, -1.2694, -0.7379, -0.6510],\n",
      "         [-0.6833,  0.8025, -0.3791, -1.2830]],\n",
      "\n",
      "        [[ 0.5835,  0.4434, -0.1102,  0.5180],\n",
      "         [ 1.5095,  0.0809, -1.4482, -1.4230],\n",
      "         [-0.2016, -0.2890,  0.2557, -1.5615]]])\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[-0.2457, -0.7202,  0.5922,  1.4378,  0.4030, -1.2694, -0.7379, -0.6510,\n",
      "         -0.6833,  0.8025, -0.3791, -1.2830],\n",
      "        [ 0.5835,  0.4434, -0.1102,  0.5180,  1.5095,  0.0809, -1.4482, -1.4230,\n",
      "         -0.2016, -0.2890,  0.2557, -1.5615]])\n",
      "torch.Size([2, 12])\n",
      "tensor([[-0.2457, -0.7202,  0.5922,  1.4378],\n",
      "        [ 0.4030, -1.2694, -0.7379, -0.6510],\n",
      "        [-0.6833,  0.8025, -0.3791, -1.2830],\n",
      "        [ 0.5835,  0.4434, -0.1102,  0.5180],\n",
      "        [ 1.5095,  0.0809, -1.4482, -1.4230],\n",
      "        [-0.2016, -0.2890,  0.2557, -1.5615]])\n",
      "torch.Size([6, 4])\n",
      "tensor([-0.2457, -0.7202,  0.5922,  1.4378,  0.4030, -1.2694, -0.7379, -0.6510,\n",
      "        -0.6833,  0.8025, -0.3791, -1.2830,  0.5835,  0.4434, -0.1102,  0.5180,\n",
      "         1.5095,  0.0809, -1.4482, -1.4230, -0.2016, -0.2890,  0.2557, -1.5615])\n",
      "torch.Size([24])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "tensor_1 = torch.randn((2,3,4))\n",
    "print(tensor_1)\n",
    "print(tensor_1.shape)\n",
    "tensor_2 = torch.flatten(tensor_1,start_dim=1)\n",
    "print(tensor_2)\n",
    "print(tensor_2.shape)\n",
    "tensor_3 = torch.flatten(tensor_1,start_dim=0,end_dim=1)\n",
    "print(tensor_3)\n",
    "print(tensor_3.shape)\n",
    "tensor_4 = torch.flatten(tensor_1,start_dim=0,end_dim=2)\n",
    "print(tensor_4)\n",
    "print(tensor_4.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        拼接与拆分: 在神经网络的前向传播过程中, 往往需要对多个分支的张量加以融合或拆分。\n",
    "        cat在CV常用，而stack在NLP常用，chunk相当于将块进行分离拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.3708,  1.3519],\n",
      "          [ 1.6196,  2.5773]],\n",
      "\n",
      "         [[ 1.7270, -0.3816],\n",
      "          [ 0.5572,  0.5273]]]])\n",
      "tensor([[[[ 0.4607,  1.4197],\n",
      "          [ 0.7299,  0.4440]],\n",
      "\n",
      "         [[-0.5487,  0.5166],\n",
      "          [ 0.3944,  0.1565]]]])\n",
      "tensor([[[[ 0.3708,  1.3519],\n",
      "          [ 1.6196,  2.5773]],\n",
      "\n",
      "         [[ 1.7270, -0.3816],\n",
      "          [ 0.5572,  0.5273]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4607,  1.4197],\n",
      "          [ 0.7299,  0.4440]],\n",
      "\n",
      "         [[-0.5487,  0.5166],\n",
      "          [ 0.3944,  0.1565]]]])\n",
      "torch.Size([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor1 = torch.randn((1,2,2,2))\n",
    "tensor2 = torch.randn((1,2,2,2))\n",
    "tensor3 = torch.cat((tensor1,tensor2),dim=0)\n",
    "#t1=[], t2=[], t3=[[],[]] \n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "print(tensor3)\n",
    "print(tensor3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        PyTorch 中封装了很多关于张量的基本计算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]]) \n",
      " tensor([[1., 1.]]) \n",
      " tensor([[ 1.7001,  0.4188],\n",
      "        [-0.3061, -0.2500]])\n",
      "tensor([[2., 2.]])\n",
      "tensor([[0., 0.]])\n",
      "tensor([[1., 1.]])\n",
      "tensor([[1., 1.]])\n",
      "tensor(1.7001) tensor(0)\n",
      "tensor(-0.3061) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(1,2)\n",
    "b = torch.ones(1,2)\n",
    "c = torch.randn(2,2)\n",
    "add = a + b # 加法运算\n",
    "sub = a - b # 减法运算\n",
    "div = a/b # 除法运算\n",
    "mul = a*b # 乘法运算 ， 也可以用@或 matmul\n",
    "max = c.max() # 求解最大元素\n",
    "am = c.argmax()\n",
    "min = c.min() # 求解最小元素\n",
    "amin = c.argmin()\n",
    "print(a,\"\\n\",b,\"\\n\",c)\n",
    "print(add)\n",
    "print(sub)\n",
    "print(div)\n",
    "print(mul)\n",
    "print(max,am)\n",
    "print(min,amin)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        自动求导\n",
    "![求导](Pic/image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.]) \n",
      " tensor([2.1259], requires_grad=True) \n",
      " tensor([0.8356], requires_grad=True) \n",
      " tensor([4.2519], grad_fn=<MulBackward0>)\n",
      "tensor([5.0874], grad_fn=<AddBackward0>)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6611/1797186573.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor([2])#定义输入张量\n",
    "# 初始化权重参数w，偏移量 b，并设置 require-grad为 True，为自动求导\n",
    "w = torch.randn(1,requires_grad=True)\n",
    "b = torch.randn(1,requires_grad=True)\n",
    "y = torch.mul(w,x)\n",
    "z = torch.add(y,b)#等价于y+b\n",
    "z.backward()#标量进行反向传播。向量需要构建梯度矩阵\n",
    "print(x,\"\\n\",w,\"\\n\",b,\"\\n\",y)\n",
    "print(z)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![求导](Pic/image2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
